{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQ9fQTjZhiTf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from scipy.spatial.distance import euclidean\n",
    "from IPython.display import Image\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "# For results repeatability\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_biG4sLLhiTp"
   },
   "source": [
    "# Dataset preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EL7GvsezhiTs"
   },
   "source": [
    "## Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUVIv7glhiTt"
   },
   "source": [
    "A function to upload and preprocess data in the \"adults\" format. As a first approach we simply remove rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "injPNctshiTu"
   },
   "outputs": [],
   "source": [
    "def get_clean_dataset1(data_file, test=False):\n",
    "    dataset = pd.read_csv(data_file, header=None)\n",
    "    dataset.columns = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"salary\"]\n",
    "    dataset = dataset[(dataset!=\" ?\").all(axis=1)].reset_index(drop=True)\n",
    "    df_strings = dataset.select_dtypes(['object'])\n",
    "    dataset[df_strings.columns] = df_strings.apply(lambda x: x.str.strip())\n",
    "    clean_dataset = pd.DataFrame(dataset[\"age\"])\n",
    "    for col in dataset.columns[1:-1]:\n",
    "        if(dataset[col].dtype =='O'):\n",
    "            clean_dataset = clean_dataset.join(pd.get_dummies(dataset[col], prefix=col))\n",
    "        else:\n",
    "            clean_dataset = clean_dataset.join(dataset[col])\n",
    "    \n",
    "    if test: labels = (dataset[\"salary\"]==\">50K.\")*1\n",
    "    else: labels = (dataset[\"salary\"]==\">50K\")*1\n",
    "    return clean_dataset, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXKmLRhRhiTx"
   },
   "source": [
    "### Training / Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_adult, labels_adult = get_clean_dataset1(\"data/adult.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5TG3DkvhiT0"
   },
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NeZO71P7hiT2"
   },
   "outputs": [],
   "source": [
    "test_dataset_adult, test_labels_adult = get_clean_dataset1(\"data/adult.test\", test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a 'missing' column in the adult test dataset because it does not contain people born in Holand-Netherlands\n",
    "# We virtually add one to be able to run all algorithms\n",
    "test_dataset_adult['native-country_Holand-Netherlands'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to visualize if some basic features one can easily think of seem to have an influence on salary (like sex or education)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_low = dataset[(dataset[\"sex\"]==\"Male\") & (dataset[\"salary\"]==\"<=50K\")].shape[0]\n",
    "male_high = dataset[(dataset[\"sex\"]==\"Male\") & (dataset[\"salary\"]==\">50K\")].shape[0]\n",
    "female_low = dataset[(dataset[\"sex\"]==\"Female\") & (dataset[\"salary\"]==\"<=50K\")].shape[0]\n",
    "female_high = dataset[(dataset[\"sex\"]==\"Female\") & (dataset[\"salary\"]==\">50K\")].shape[0]\n",
    "fisher_tab = np.array([[male_low, male_high], [female_low, female_high]])\n",
    "\n",
    "barWidth = 0.2\n",
    "bars1 = [female_high, male_high]\n",
    "bars2 = [female_low, male_low]\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    " \n",
    "plt.bar(r1, bars1, width = barWidth, color = 'blue', edgecolor = 'black',label='>=50K')\n",
    "plt.bar(r2, bars2, width = barWidth, color = 'cyan', edgecolor = 'black', label='<50K')\n",
    "\n",
    "plt.xticks([r + barWidth for r in range(len(bars1))], ['Female', 'Male'])\n",
    "plt.legend()\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_low = dataset[(dataset[\"native-country\"]==\"United-States\") & (dataset[\"salary\"]==\"<=50K\")].shape[0]\n",
    "US_high = dataset[(dataset[\"native-country\"]==\"United-States\") & (dataset[\"salary\"]==\">50K\")].shape[0]\n",
    "non_US_low = dataset[(dataset[\"native-country\"]!=\"United-States\") & (dataset[\"salary\"]==\"<=50K\")].shape[0]\n",
    "non_US_high = dataset[(dataset[\"native-country\"]!=\"United-States\") & (dataset[\"salary\"]==\">50K\")].shape[0]\n",
    "barWidth = 0.2\n",
    "bars1 = [US_high, non_US_high]\n",
    "bars2 = [US_low, US_high]\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    " \n",
    "plt.bar(r1, bars1, width = barWidth, color = 'blue', edgecolor = 'black',label='>=50K')\n",
    "plt.bar(r2, bars2, width = barWidth, color = 'cyan', edgecolor = 'black', label='<50K')\n",
    "\n",
    "plt.xticks([r + barWidth for r in range(len(bars1))], ['US', 'Non US'])\n",
    "plt.legend()\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgsWYitGhiT5"
   },
   "source": [
    "## Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVCg0XtghiT7"
   },
   "outputs": [],
   "source": [
    "def get_clean_dataset2(data_file):\n",
    "    dataset = pd.read_csv(data_file, header=None)\n",
    "    dataset.columns = [\"white_king_column\",\"white_king_row\",\"white_rook_column\",\"white_rook_row\",\"black_king_column\",\"black_king_row\", \"outcome\"]\n",
    "    df_strings = dataset.select_dtypes(['object'])\n",
    "    dataset[df_strings.columns] = df_strings.apply(lambda x: x.str.strip())\n",
    "    clean_dataset = pd.DataFrame()\n",
    "    for col in dataset.columns[0:-1]:\n",
    "        if(dataset[col].dtype =='O'):\n",
    "            clean_dataset = clean_dataset.join(pd.get_dummies(dataset[col], prefix=col))\n",
    "        else:\n",
    "            clean_dataset = clean_dataset.join(dataset[col])\n",
    "\n",
    "    labels = (dataset[\"outcome\"]!=\"draw\")*1\n",
    "    return clean_dataset, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6sRVV7ehiT7"
   },
   "source": [
    "### Training / Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RP_VMe4khiT8"
   },
   "outputs": [],
   "source": [
    "dataset_chess, labels_chess = get_clean_dataset2(\"data/krkopt.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGWdZarShiT_"
   },
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKkr73QFhiT_"
   },
   "outputs": [],
   "source": [
    "test_dataset_chess, test_labels_chess = get_clean_dataset2(\"data/krkopt.test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tmeCexehiUA"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqWPXTInhiUA"
   },
   "outputs": [],
   "source": [
    "def plotMeanAndStd(stats, x, color='b', ax = None, legend=None):\n",
    "    \"\"\"\n",
    "    Input : array of tuples (mean std) and their x coordinates\n",
    "    \"\"\"\n",
    "    mean = np.array([s[0] for s in stats])\n",
    "    standard_dev = np.array([s[1] for s in stats])\n",
    "    if ax == None :\n",
    "        plt.plot(x, mean, c=color,label=legend)\n",
    "        plt.fill_between(x, mean-standard_dev, mean+standard_dev, alpha=0.2, color=color)\n",
    "    else:\n",
    "        ax.plot(x, mean, c=color,label=legend)\n",
    "        ax.fill_between(x, mean-standard_dev, mean+standard_dev, alpha=0.2, color=color)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOLE_2SPhiUB"
   },
   "outputs": [],
   "source": [
    "def error(predicted_labels, real_labels, loss=\"euclidean\"):\n",
    "    \"\"\"\n",
    "    Input: numpy array containing respectively the labels an algorithm predicted, and the real labels corresponding\n",
    "    to the data. Type of loss we want to use.\n",
    "    \n",
    "    Output: float, the computed loss.\n",
    "    \"\"\"\n",
    "    if loss == \"euclidean\": return euclidean(predicted_labels, real_labels)\n",
    "    elif loss == \"manhattan\": return sum(abs(predicted_labels - real_labels))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Givn177fhiUC"
   },
   "source": [
    "### Features normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the possibility to perform data normalization with scikit-learn's scaler to avoid having overweighted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataset):\n",
    "    columns = dataset.columns\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(dataset)\n",
    "    dataset[columns] = scaler.transform(dataset[columns])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_features(dataset, weights):\n",
    "    \"\"\"\n",
    "    Input: a dataset (pandas dataframe), and a dictionary giving each dataset's feature a weight\n",
    "    according to their importance\n",
    "    \n",
    "    Output: None, but dataset's features have been multiplied by their weight to take into account the\n",
    "    differences in their importance\n",
    "    \"\"\"\n",
    "    for feature in weights:\n",
    "        dataset[feature] *= weights[feature]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GZtGu3vhiUC"
   },
   "outputs": [],
   "source": [
    "def cross_validation(algo, dataset_, labels_, loss_=\"manhattan\", folds = 5, algo_kwargs={}):\n",
    "    \"\"\"\n",
    "    Input : Predictor function that works by supplying training set and labels and test set and return predicted labels\n",
    "            dataset  and corresponding labels\n",
    "            folds\n",
    "            algo_kwargs : a dict with additional params for the algo : ex. {'n_neighbors':5}\n",
    "    Output : Precision mean and variance\n",
    "    \"\"\"\n",
    "    dataset_size = dataset_.shape[0]\n",
    "    group_ids = np.tile(np.arange(folds),int(dataset_size/folds)+1)[:dataset_size]\n",
    "    np.random.shuffle(group_ids)\n",
    "    training_precisions = []\n",
    "    validation_precisions = []\n",
    "    for N in range(folds):\n",
    "        training_set = dataset_[group_ids != N]\n",
    "        training_labels = labels_[group_ids != N]\n",
    "        test_set = dataset_[group_ids == N]\n",
    "        test_labels = labels_[group_ids == N]\n",
    "        # Training error\n",
    "        training_predicted_labels = algo(training_set, training_labels, training_set, **algo_kwargs)\n",
    "        training_precisions += [(len(training_labels) - error(training_predicted_labels, training_labels, loss = loss_))/len(training_labels)]\n",
    "    \n",
    "        \n",
    "        # Validation error\n",
    "        validation_predicted_labels = algo(training_set, training_labels, test_set, **algo_kwargs)\n",
    "        validation_precisions += [(len(test_labels) - error(validation_predicted_labels, test_labels, loss = loss_))/len(test_labels)]\n",
    "    \n",
    "    \n",
    "    return (np.mean(training_precisions), np.std(training_precisions), np.mean(validation_precisions), np.std(validation_precisions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_size_influence(algo, dataset_, labels_, Ns=[], loss_=\"manhattan\", folds = 5, algo_kwargs={}, show_time=False, visualize=True):\n",
    "    \n",
    "    nrows = dataset_.shape[0]\n",
    "    mean_trainings = []\n",
    "    std_trainings = []\n",
    "    mean_validations = []\n",
    "    std_validations = []\n",
    "    \n",
    "    if show_time: times = []\n",
    "    \n",
    "    for N in Ns:\n",
    "        indices = list(np.random.choice(nrows, N))\n",
    "        d = dataset.iloc[indices]\n",
    "        l = labels.iloc[indices]\n",
    "        \n",
    "        if show_time: t = time.time()\n",
    "        mean_training, std_training, mean_validation, std_validation = cross_validation(knn, d, l, folds = folds, algo_kwargs=algo_kwargs)\n",
    "        if show_time: times.append(time.time() - t)\n",
    "        \n",
    "        mean_trainings.append(mean_training)\n",
    "        std_trainings.append(std_training)\n",
    "        mean_validations.append(mean_validation)\n",
    "        std_validations.append(std_validation)\n",
    "    \n",
    "    if visualize:\n",
    "        fig = plt.figure(figsize=(10,7))\n",
    "        fig.suptitle(\"Influence of training/validation dataset size\")\n",
    "        ax1 = fig.gca()\n",
    "        plotMeanAndStd([i for i in zip(mean_trainings, std_trainings)],Ns, ax= ax1, legend = \"Training\", color='red')\n",
    "        plotMeanAndStd([i for i in zip(mean_validations, std_validations)],Ns, ax= ax1, legend = \"Validation\",color='blue')\n",
    "        ax1.legend(loc=\"lower right\")\n",
    "        ax1.set_ylabel(\"Precision\")\n",
    "        ax1.set_xlabel(\"Dataset sample size\")\n",
    "        ax1.grid()\n",
    "\n",
    "        if show_time:\n",
    "            ax5 = ax1.twinx()\n",
    "            ax5.set_ylabel(\"Computation time (s)\")\n",
    "            ax5.plot(Ns, times, label =\"Computation time\", c=\"orange\")\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    if show_time: return (mean_trainings, std_trainings, mean_validations, std_validations, times)\n",
    "    else: return (mean_trainings, std_trainings, mean_validations, std_validations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCANGo53hiUE"
   },
   "source": [
    "# Knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFbCGSeXhiUF"
   },
   "source": [
    "### Knn algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use scikit-learn's knn function to design a knn classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHhrp7RchiUF"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUIengFqhiUG"
   },
   "outputs": [],
   "source": [
    "def knn(training_features, training_labels, to_predict_features,\n",
    "        n_neighbors=5, weights = \"uniform\", algorithm=\"auto\", p=2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: Training data, features for which we want to predict the labels, number of neighbors k for knn algo,\n",
    "    features weights system ('uniform' or 'distance'), algorithm usewd to find closer k neighbors, p is the value\n",
    "    used in the computation of the minkowski distance that is used here, p=1 gives a manhattan distance, p=2 a\n",
    "    euclidian distance.\n",
    "    \n",
    "    Output: Numpy array containing the labels predicted by KNN for the given 'to_predict_features'\n",
    "    \"\"\"\n",
    "    neigh = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm, p=p)\n",
    "    neigh.fit(training_features, training_labels)\n",
    "    \n",
    "    return neigh.predict(to_predict_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn8B6ekvhiUH"
   },
   "source": [
    "### Let us study the influence of the hyperparameter K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us study the influence of the hyperparameter k (number of neighbors) on KNN algorithm's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAmoxWKZhiUH"
   },
   "outputs": [],
   "source": [
    "def n_neighbors_influence_multiple_datasize(Ks, dataset_, labels_, Ns, loss_=\"manhattan\",\n",
    "                                            weights = \"uniform\", folds = 5, show_time=False, \n",
    "                                            visualize=True):\n",
    "    \n",
    "    mean_training_per_k = dict()\n",
    "    std_training_per_k = dict()\n",
    "    mean_validation_per_k = dict()\n",
    "    std_validation_per_k = dict()\n",
    "    \n",
    "    if show_time: time_per_k = dict()\n",
    "    \n",
    "    for k in Ks:\n",
    "        print(k)\n",
    "        if show_time: (mean_trainings, std_trainings, mean_validations, std_validations, times) = data_size_influence(knn, dataset_, labels_, Ns, loss_=\"manhattan\", folds = folds, algo_kwargs={\"n_neighbors\":k, \"weights\":weights}, show_time=show_time, visualize=False)\n",
    "        else: (mean_trainings, std_trainings, mean_validations, std_validations) = data_size_influence(knn, dataset_, labels_, Ns, loss_=\"manhattan\", folds = folds, algo_kwargs={\"n_neighbors\":k, \"weights\":weights}, show_time=show_time, visualize=False)\n",
    "            \n",
    "        mean_training_per_k[k] = mean_trainings\n",
    "        std_training_per_k[k] = std_trainings\n",
    "        mean_validation_per_k[k]  = mean_validations\n",
    "        std_validation_per_k[k]  = std_validations\n",
    "\n",
    "        if show_time: time_per_k[k]  = times\n",
    "    subplotcode = 120\n",
    "    if show_time :\n",
    "        subplotcode = 130\n",
    "    if visualize:\n",
    "        \n",
    "        fig = plt.figure(figsize=(15,10))\n",
    "        fig.suptitle(\"Influence of K (in k nearest neighbor) and training/validation dataset size\")\n",
    "        \n",
    "        ax1 = fig.add_subplot(subplotcode+1)\n",
    "        ax1.title.set_text(\"Training precision\")\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        curveID = 0\n",
    "        for k in Ks:\n",
    "            plotMeanAndStd([i for i in zip(mean_training_per_k[k],std_training_per_k[k])],Ns, legend=\"K = \"+str(k),ax = ax1, color = cmap(curveID))\n",
    "            curveID += 1\n",
    "        ax1.set_ylim(0,1.05)\n",
    "        ax1.set_xlim(Ns[0],Ns[-1])\n",
    "        ax1.grid()\n",
    "        ax1.legend()\n",
    "        \n",
    "        ax2 = fig.add_subplot(subplotcode+2)\n",
    "        curveID = 0\n",
    "        for k in Ks:\n",
    "            plotMeanAndStd([i for i in zip(mean_validation_per_k[k],std_validation_per_k[k])],Ns, legend=\"K = \"+str(k), ax=ax2,color = cmap(curveID))\n",
    "            curveID += 1\n",
    "        ax2.set_ylim(0,1.05)\n",
    "        ax2.title.set_text(\"Validation precision\")\n",
    "        ax2.set_xlim(Ns[0],Ns[-1])\n",
    "        ax2.grid()\n",
    "        ax2.legend()\n",
    "        \n",
    "        if show_time:\n",
    "            ax5 = fig.add_subplot(subplotcode+3)\n",
    "            ax5.title.set_text(\"Computation time\")\n",
    "            for k in Ks:\n",
    "                ax5.plot(Ns, time_per_k[k], label=f\"k={k}\")\n",
    "            ax5.grid()\n",
    "            plt.legend()\n",
    "        \n",
    "\n",
    "\n",
    "        fig.tight_layout()\n",
    "              \n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    if show_time: return (mean_training_per_k, std_training_per_k, mean_validation_per_k, std_validation_per_k, time_per_k)\n",
    "    else: return (mean_training_per_k, std_training_per_k, mean_validation_per_k, std_validation_per_k)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dt4zxc5thiUJ"
   },
   "outputs": [],
   "source": [
    "nrows = dataset_adult.shape[0]\n",
    "Ks = list(range(1, 50, 2))\n",
    "Ns = [int(k * nrows) for k in [0.25, 0.5, 0.75, 1]]\n",
    "\"\"\"\n",
    "n_neighbors_influence_multiple_datasize(Ks, dataset_adult, labels_adult, Ns, loss_=\"manhattan\",\n",
    "                                            folds = 5, show_time=True, visualize=True)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"img/dataset_size_influence.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the above results, when working on hyperpara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53lnubXDhiUK"
   },
   "outputs": [],
   "source": [
    "def n_neighbors_influence_fixed_datasize(Ks, dataset_, labels_, N, loss_=\"manhattan\",\n",
    "                                         weights = \"uniform\",folds = 5, show_time=False,\n",
    "                                         visualize=True):\n",
    "    nrows = dataset_.shape[0]\n",
    "    mean_trainings = []\n",
    "    std_trainings = []\n",
    "    mean_validations = []\n",
    "    std_validations = []\n",
    "    \n",
    "    if show_time: times = []\n",
    "    \n",
    "    for k in Ks:\n",
    "        print(k)\n",
    "        indices = list(np.random.choice(nrows, N))\n",
    "        d = dataset_.iloc[indices]\n",
    "        l = labels_.iloc[indices]\n",
    "        \n",
    "        if show_time: t = time.time()\n",
    "        mean_training, std_training, mean_validation, std_validation = cross_validation(knn, d, l, folds = folds, algo_kwargs={\"n_neighbors\": k, \"weights\":weights})\n",
    "        if show_time: times.append(time.time() - t)\n",
    "        \n",
    "        mean_trainings.append(mean_training)\n",
    "        std_trainings.append(std_training)\n",
    "        mean_validations.append(mean_validation)\n",
    "        std_validations.append(std_validation)\n",
    "    \n",
    "    if visualize:\n",
    "        fig = plt.figure(figsize=(15,10))\n",
    "        #fig.suptitle(\"Influence of training/validation dataset size\")\n",
    "        ax1 = fig.gca()\n",
    "        plotMeanAndStd([i for i in zip(mean_trainings, std_trainings)], Ks, ax = ax1, color = 'b', legend=\"Training\")\n",
    "        plotMeanAndStd([i for i in zip(mean_validations, std_validations)], Ks, ax = ax1, color = 'r', legend=\"Validation\")\n",
    "        ax1.set_ylim(0,1.1)\n",
    "        ax1.grid()\n",
    "        ax1.title.set_text(\"Precision\")\n",
    "        ax1.set_xlabel(\"K\")\n",
    "        ax1.set_ylabel(\"Precision\")\n",
    "        ax1.legend()\n",
    "        ax1.set_xlim(min(Ks), max(Ks))\n",
    "        if show_time:\n",
    "            ax5 = fig.add_subplot(235)\n",
    "            ax5.title.set_text(\"Computation time\")\n",
    "            ax5.plot(Ks, times)\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    if show_time: return (mean_trainings, std_trainings, mean_validations, std_validations, times)\n",
    "    else: \n",
    "        return (mean_trainings, std_trainings, mean_validations, std_validations)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RflEuMffhiUM"
   },
   "source": [
    "### Knn results for dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a normalized version of the dataset, as well as a version where, after normalization, features are weighted according to the importance a decision tree algorithm gives them. As a trade-off between computation time and precision, we only work with a subset whose size is 75% of the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = dataset_adult.shape[0]\n",
    "N = int(0.75 * nrows)\n",
    "indices = list(np.random.choice(nrows, N))\n",
    "dataset_adult_75 = dataset_adult.iloc[indices]\n",
    "labels_adult_75 = labels_adult.iloc[indices]\n",
    "\n",
    "normalized_dataset_adult = dataset_adult_75.copy(deep=True)\n",
    "normalize(normalized_dataset_adult)\n",
    "\n",
    "normalized_test_dataset_adult = test_dataset_adult.copy(deep=True)\n",
    "normalize(normalized_test_dataset_adult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_dataset_adult = normalized_dataset_adult.copy(deep=True)\n",
    "weights = features_importance(normalized_dataset_adult, labels_adult_75, criterion='gini', splitter='best')\n",
    "weight_features(weighted_dataset_adult, weights)\n",
    "\n",
    "\n",
    "weighted_test_dataset_adult = normalized_test_dataset_adult.copy(deep=True)\n",
    "weight_features(weighted_test_dataset_adult, weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mmQqwBWhiUM"
   },
   "source": [
    "#### Selecting best hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Uniform weights, no normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5q8G-pJchiUL"
   },
   "outputs": [],
   "source": [
    "(mean_trainings, std_trainings, mean_validations, std_validations) = n_neighbors_influence_fixed_datasize(list(range(1, 15)), dataset_adult_75, labels_adult_75, N, loss_=\"manhattan\",\n",
    "                                     weights=\"uniform\", folds = 5, show_time=False, visualize=True)\n",
    "print(f\"Best result is {max(mean_validations)} for k = {mean_validations.index(max(mean_validations)) + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the above results, we choose to use the hyperparameter k =."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Uniform weights, normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mean_trainings, std_trainings, mean_validations, std_validations) = n_neighbors_influence_fixed_datasize(list(range(1, 15)), normalized_dataset_adult, labels_adult, N, loss_=\"manhattan\",\n",
    "                                     weights=\"uniform\", folds = 5, show_time=False, visualize=True)\n",
    "print(f\"Best result is {max(mean_validations)} for k = {mean_validations.index(max(mean_validations)) + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the above results, we choose to use the hyperparameter k =."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Uniform weights, normalization and weighted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mean_trainings, std_trainings, mean_validations, std_validations) =n_neighbors_influence_fixed_datasize(list(range(1, 15)), weighted_dataset_adult, labels_adult, N, loss_=\"manhattan\",\n",
    "                                     weights=\"uniform\", folds = 5, show_time=False, visualize=True)\n",
    "\n",
    "print(f\"Best result is {max(mean_validations)} for k = {mean_validations.index(max(mean_validations)) + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the above results, we choose to use the hyperparameter k =."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Distance weights, no normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mean_trainings, std_trainings, mean_validations, std_validations) =n_neighbors_influence_fixed_datasize(list(range(1, 15)), dataset_adult_75, labels_adult_75, N, loss_=\"manhattan\",\n",
    "                                     weights=\"distance\", folds = 5, show_time=False, visualize=True)\n",
    "print(f\"Best result is {max(mean_validations)} for k = {mean_validations.index(max(mean_validations)) + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the above results, we choose to use the hyperparameter k =."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Distance weights, normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mean_trainings, std_trainings, mean_validations, std_validations) =n_neighbors_influence_fixed_datasize(list(range(1, 15)), normalized_dataset_adult, labels_adult, N, loss_=\"manhattan\",\n",
    "                                     weights=\"distance\", folds = 5, show_time=False, visualize=True)\n",
    "print(f\"Best result is {max(mean_validations)} for k = {mean_validations.index(max(mean_validations)) + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the above results, we choose to use the hyperparameter k =."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Distance weights, normalization and weighted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mean_trainings, std_trainings, mean_validations, std_validations) =n_neighbors_influence_fixed_datasize(list(range(1, 15)), weighted_dataset_adult, labels_adult, N, loss_=\"manhattan\",\n",
    "                                     weights=\"uniform\", folds = 5, show_time=False, visualize=True)\n",
    "print(f\"Best result is {max(mean_validations)} for k = {mean_validations.index(max(mean_validations)) + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the above results, we choose to use the hyperparameter k =."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yItfwVREhiUN"
   },
   "source": [
    "#### Results on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uniform weights, no normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels1 = knn(dataset_adult_75, labels_adult_75, test_dataset_adult, n_neighbors=k1, weights = \"uniform\", algorithm=\"auto\", p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision1 = (len(test_labels_adult) - error(predicted_labels1, test_labels_adult))/len(test_labels_adult)\n",
    "print(f\"Precision on the test dataset is {precision1}\")\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test_labels_adult, predicted_labels1))\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uniform weights, normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels2 = knn(normalized_dataset_adult, labels_adult_75, normalized_test_dataset_adult, n_neighbors=k2, weights = \"uniform\", algorithm=\"auto\", p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision2 = (len(test_labels_adult) - error(predicted_labels2, test_labels_adult))/len(test_labels_adult)\n",
    "print(f\"Precision on the test dataset is {precision2}\"\")\n",
    "ConfusionMatrixDisplay.from_predictions(test_labels_adult, predicted_labels2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uniform weights, normalization and weighted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k3 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels3 = knn(weighted_dataset_adult, labels_adult_75, weighted_test_dataset_adult, n_neighbors=k3, weights = \"uniform\", algorithm=\"auto\", p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision3 = (len(test_labels_adult) - error(predicted_labels3, test_labels_adult))/len(test_labels_adult)\n",
    "print(f\"Precision on the test dataset is {precision3}\"\")\n",
    "ConfusionMatrixDisplay.from_predictions(test_labels_adult, predicted_labels3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance weights, no normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k4 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels4 = knn(dataset_adult, labels_adult_75, test_dataset_adult, n_neighbors=k4, weights = \"distance\", algorithm=\"auto\", p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision4 = (len(test_labels_adult) - error(predicted_labels4, test_labels_adult))/len(test_labels_adult)\n",
    "print(f\"Precision on the test dataset is {precision4}\"\")\n",
    "ConfusionMatrixDisplay.from_predictions(test_labels_adult, predicted_labels4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance weights,  normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k5 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels5 = knn(normalized_dataset_adult, labels_adult_75, normalized_test_dataset_adult, n_neighbors=k5, weights = \"distance\", algorithm=\"auto\", p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision5 = (len(test_labels_adult) - error(predicted_labels5, test_labels_adult))/len(test_labels_adult)\n",
    "print(f\"Precision on the test dataset is {precision5}\"\")\n",
    "ConfusionMatrixDisplay.from_predictions(test_labels_adult, predicted_labels5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance weights,  normalization and weighted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k6 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels6 = knn(weighted_dataset_adult, labels_adult_75, weighted_test_dataset_adult, n_neighbors=k6, weights = \"distance\", algorithm=\"auto\", p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision6 = (len(test_labels_adult) - error(predicted_labels6, test_labels_adult))/len(test_labels_adult)\n",
    "print(f\"Precision on the test dataset is {precision6}\"\")\n",
    "ConfusionMatrixDisplay.from_predictions(test_labels_adult, predicted_labels6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvvhRXUfhiUO"
   },
   "outputs": [],
   "source": [
    "# TO DO : print precision of test dataset, add confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvxZzs7EhiUO"
   },
   "source": [
    "### Knn results for dataset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a normalized version of the dataset, as well as a version where, after normalization, features are weighted according to the importance a decision tree algorithm gives them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dataset_chess = dataset_chess.copy(deep=True)\n",
    "normalize(normalized_dataset_chess)\n",
    "\n",
    "normalized_test_dataset_chess = test_dataset_chess.copy(deep=True)\n",
    "normalize(normalized_test_dataset_chess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_dataset_chess = normalized_dataset_chess.copy(deep=True)\n",
    "weights = features_importance(normalized_dataset_chess, labels_chess, criterion='gini', splitter='best')\n",
    "weight_features(weighted_dataset_chess, weights)\n",
    "\n",
    "\n",
    "weighted_test_dataset_chess = normalized_test_dataset_chess.copy(deep=True)\n",
    "weight_features(weighted_test_dataset_chess, weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TB9oc6y0hiUP"
   },
   "source": [
    "#### Selecting best hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uniform weights, no normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mean_trainings, std_trainings, mean_validations, std_validations) =n_neighbors_influence_fixed_datasize(list(range(1, 50)), dataset_chess, labels_chess, nrows, loss_=\"manhattan\",\n",
    "                                     weights=\"uniform\", folds = 5, show_time=False, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the above results, we choose to use the hyperparameter k =."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uniform weights, normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mean_trainings, std_trainings, mean_validations, std_validations) =n_neighbors_influence_fixed_datasize(list(range(1, 50)), normalized_dataset_chess, labels_chess, nrows, loss_=\"manhattan\",\n",
    "                                     weights=\"uniform\", folds = 5, show_time=False, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the above results, we choose to use the hyperparameter k =."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uniform weights,  normalization and weighted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mean_trainings, std_trainings, mean_validations, std_validations) =n_neighbors_influence_fixed_datasize(list(range(1, 50)), weighted_dataset_chess, labels_chess, nrows, loss_=\"manhattan\",\n",
    "                                     weights=\"uniform\", folds = 5, show_time=False, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the above results, we choose to use the hyperparameter k =."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance weights, no normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mean_trainings, std_trainings, mean_validations, std_validations) =n_neighbors_influence_fixed_datasize(list(range(1, 50)), dataset_chess, labels_chess, nrows, loss_=\"manhattan\",\n",
    "                                     weights=\"distance\", folds = 5, show_time=False, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the above results, we choose to use the hyperparameter k =."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance weights, normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mean_trainings, std_trainings, mean_validations, std_validations) =n_neighbors_influence_fixed_datasize(list(range(1, 50)), normalized_dataset_chess, labels_chess, nrows, loss_=\"manhattan\",\n",
    "                                     weights=\"distance\", folds = 5, show_time=False, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the above results, we choose to use the hyperparameter k =."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance weights, normalization and weighted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mean_trainings, std_trainings, mean_validations, std_validations) =n_neighbors_influence_fixed_datasize(list(range(1, 50)), weighted_dataset_chess, labels_chess, nrows, loss_=\"manhattan\",\n",
    "                                     weights=\"distance\", folds = 5, show_time=False, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the above results, we choose to use the hyperparameter k =."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBchGgoshiUP"
   },
   "outputs": [],
   "source": [
    "# TO DO : plot the performance depending on K for the entire dataset, choose simplest best K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_vQSdO2hiUQ"
   },
   "source": [
    "#### Results on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uniform weights, no normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels1 = knn(dataset_chess, labels_chess, test_dataset_chess, n_neighbors=k1, weights = \"uniform\", algorithm=\"auto\", p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision1 = (len(test_labels_chess) - error(predicted_labels1, test_labels_chess))/len(test_labels_chess)\n",
    "print(f\"Precision on the test dataset is {precision1}\"\")\n",
    "ConfusionMatrixDisplay.from_predictions(test_labels_chess, predicted_labels1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uniform weights, normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels2 = knn(normalized_dataset_chess, labels_chess, normalized_test_dataset_chess, n_neighbors=k2, weights = \"uniform\", algorithm=\"auto\", p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision2 = (len(test_labels_chess) - error(predicted_labels2, test_labels_chess))/len(test_labels_chess)\n",
    "print(f\"Precision on the test dataset is {precision2}\")\n",
    "ConfusionMatrixDisplay.from_predictions(test_labels_chess, predicted_labels2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uniform weights, normalization and weighted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k3 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels3 = knn(weighted_dataset_adult, labels_adult, weighted_test_dataset_adult, n_neighbors=k3, weights = \"uniform\", algorithm=\"auto\", p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision3 = (len(test_labels_chess) - error(predicted_labels3, test_labels_chess))/len(test_labels_chess)\n",
    "print(f\"Precision on the test dataset is {precision3}\")\n",
    "ConfusionMatrixDisplay.from_predictions(test_labels_chess, predicted_labels3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance weights, no normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k4 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels4 = knn(dataset_chess, labels_chess, test_dataset_chess, n_neighbors=k4, weights = \"distance\", algorithm=\"auto\", p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision4 = (len(test_labels_chess) - error(predicted_labels4, test_labels_chess))/len(test_labels_chess)\n",
    "print(f\"Precision on the test dataset is {precision4}\"\")\n",
    "ConfusionMatrixDisplay.from_predictions(test_labels_chess, predicted_labels4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance weights,  normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k5 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels5 = knn(normalized_dataset_chess, labels_chess, normalized_test_dataset_chess, n_neighbors=k5, weights = \"distance\", algorithm=\"auto\", p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision5 = (len(test_labels_chess) - error(predicted_labels5, test_labels_chess))/len(test_labels_chess)\n",
    "print(f\"Precision on the test dataset is {precision5}\"\")\n",
    "ConfusionMatrixDisplay.from_predictions(test_labels_chess, predicted_labels5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance weights,  normalization and weighted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k6 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels6 = knn(weighted_dataset_chess, labels_chess, weighted_test_dataset_chess, n_neighbors=k6, weights = \"distance\", algorithm=\"auto\", p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision6 = (len(test_labels_chess) - error(predicted_labels6, test_labels_chess))/len(test_labels_chess)\n",
    "print(f\"Precision on the test dataset is {precision6}\"\")\n",
    "ConfusionMatrixDisplay.from_predictions(test_labels_chess, predicted_labels6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvvhRXUfhiUO"
   },
   "outputs": [],
   "source": [
    "# TO DO : print precision of test dataset, add confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjfDL3_NhiUQ"
   },
   "outputs": [],
   "source": [
    "# TO DO : print precision of test dataset, add confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGgE3GtJhiUQ"
   },
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1czo2QT4hiUR"
   },
   "source": [
    "We use scikit-learn's decision tree function to design a decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uO9tUgiNhiUR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HEiu5H3hiUR"
   },
   "outputs": [],
   "source": [
    "def decision_tree(training_features, training_labels, to_predict_features,\n",
    "                  criterion='gini', splitter='best'):\n",
    "    \"\"\"\n",
    "    :param training_features: training features (x)\n",
    "    :param training_labels: training labels (y)\n",
    "    :param to_predict_features: features that we want to predict\n",
    "    :param criterion: {\"best\", \"random\"} Default is \"gini\" for the Gini impurity and \"entropy\" for the information gain\n",
    "    :param splitter: {\"best\", \"random\"} Default is \"best\" to choose the best split and \"random\" to choose the best random split.\n",
    "    :return:Numpy array containing the labels predicted by Decision Tree for the given 'to_predict_features'\n",
    "    \"\"\"\n",
    "    classifier = DecisionTreeClassifier(criterion=criterion, splitter=splitter)\n",
    "    classifier.fit(training_features, training_labels)\n",
    "    \n",
    "    return classifier.predict(to_predict_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LsA0O9nIhiUR"
   },
   "outputs": [],
   "source": [
    "# TO DO : see influence of cost function on precision (same graph as for KNN : dataset size on x axis, one trace for each cost function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgZFWL9zhiUS"
   },
   "outputs": [],
   "source": [
    "# TO DO : see influence of max depth on precision (same graph as for KNN : fixed data set size, max depth on x axis, precision on y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "104uJclihiUS"
   },
   "source": [
    "### Decision tree results for dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UElk887whiUS"
   },
   "source": [
    "##### Selecting best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBxF9uTRhiUT"
   },
   "outputs": [],
   "source": [
    "#TO DO plot precision depending on max depth, one line for each cost function, selecting simplest best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ANuC-VchiUT"
   },
   "source": [
    "##### Results on dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8Y6ADh_hiUT"
   },
   "outputs": [],
   "source": [
    "# Print precision and confusion matrix on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnGbkZlrhiUU"
   },
   "source": [
    "### Decision tree results for dataset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcFCI5xkhiUU"
   },
   "source": [
    "##### Selecting best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMWoniQkhiUU"
   },
   "outputs": [],
   "source": [
    "#TO DO plot precision depending on max depth, one line for each cost function, selecting simplest best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hpd4LWoghiUU"
   },
   "source": [
    "#### Results on dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WBxebSAhiUV"
   },
   "outputs": [],
   "source": [
    "# Print precision and confusion matrix on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvvzULNjhiUV"
   },
   "source": [
    "### Identify the features considered as most important from decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this section is to be able to identify the data features that decision trees identify as most signficant. This could be useful, in particular to weight features according to their importance when performing KNN classication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_importance(training_features, training_labels, criterion='gini',\n",
    "                        splitter='best', visualize = False, nb_top=5):\n",
    "    \"\"\"\n",
    "    Input: training features and labels, decision tree's parameters, whether we want to visualize feature's\n",
    "    importance\n",
    "    \n",
    "    Output: a dictionary giving each feature a score according to its importance in the decision tree\n",
    "    \"\"\"\n",
    "    \n",
    "    classifier = DecisionTreeClassifier(criterion=criterion, splitter=splitter)\n",
    "    classifier.fit(training_features, training_labels)\n",
    "    features_importances = classifier.feature_importances_\n",
    "    \n",
    "    importance = dict()\n",
    "    cols = training_features.columns\n",
    "    for i,v in enumerate(features_importances):\n",
    "        importance[cols[i]] = v\n",
    "    \n",
    "    if visualize:\n",
    "        # Say which features are most important\n",
    "        ordered_features = sorted(importance.keys(), key = lambda k:(importance[k])) \n",
    "        ordered_features.reverse()\n",
    "        for i in range(nb_top):\n",
    "            feature = ordered_features[i]\n",
    "            print(f\"Feature ranked {i + 1} is {feature} with score {importance[feature]}\")\n",
    "            \n",
    "        # Plot feature importance\n",
    "        plt.bar([x for x in range(len(features_importances))], features_importances)\n",
    "        plt.show()\n",
    "    \n",
    "    return importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of dataset's size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzYZsoQlhiUV"
   },
   "source": [
    "Now we study the influence of the size of the training / validation dataset on the training / validation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHaJgF0ohiUV"
   },
   "outputs": [],
   "source": [
    "data_size_influence(knn, dataset, labels, 50, 10000, 1000, loss_=\"manhattan\", folds = 5, algo_kwargs={\"n_neighbors\":3}, show_time=True, visualize=True)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KF22sDhFhiUX"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuRHWlRBhiUY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yHI7yS5hiUY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Homework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
